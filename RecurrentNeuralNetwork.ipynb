{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RecurrentNeuralNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4p171Skeabnd4Eul0KIAR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bee-llel/Machine-Learning/blob/master/RecurrentNeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0mapvIp7xp9"
      },
      "source": [
        "#librairy:\r\n",
        "import pandas as pd # used to manipulate the data \r\n",
        "import numpy as np\r\n",
        "import scipy as sp \r\n",
        "from numpy import *\r\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPAN-qGVBHYo"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F7R-kTtxyYH",
        "outputId": "dd8843bc-2ca1-4c45-cd3d-91260d6e41ff"
      },
      "source": [
        "import random as rd \r\n",
        "from random import *\r\n",
        "#allows us to always have the same result when calling a random function\r\n",
        "np.random.seed(23)\r\n",
        "# this small part was made with another group in class ( it's the lab) so it's the same as a few classmate.\r\n",
        "\r\n",
        "#######################################\r\n",
        "# DATA : random binary number \r\n",
        "\r\n",
        "X_train = []\r\n",
        "X_test = [] \r\n",
        "y_train = []\r\n",
        "y_test = [] \r\n",
        "\r\n",
        "for i in range(30) :\r\n",
        "  X = [randint(0, 1) for i in range(10) ]\r\n",
        "  X_train.append(X)\r\n",
        "  y_train.append(sum(X))\r\n",
        "\r\n",
        "for i in range(10):\r\n",
        "  X = [randint(0, 1) for i in range(10) ]\r\n",
        "  X_test.append(X)\r\n",
        "  y_test.append(sum(X))\r\n",
        "\r\n",
        "X_train = np.asarray(X_train)\r\n",
        "X_test = np.asarray(X_test)\r\n",
        "y_train = np.asarray(y_train)\r\n",
        "y_test = np.asarray(y_test)\r\n",
        "\r\n",
        "y_train.reshape((30,1))\r\n",
        "print(X_train)\r\n",
        "print(y_train)\r\n",
        "print(y_train.shape)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 1 0 1 0 1 1 1 1]\n",
            " [0 1 0 0 1 1 0 1 0 1]\n",
            " [1 1 0 0 1 1 1 0 0 1]\n",
            " [0 1 1 1 0 1 1 0 0 0]\n",
            " [1 0 1 0 1 0 0 0 0 1]\n",
            " [0 1 0 1 1 1 0 0 0 1]\n",
            " [0 1 0 0 0 0 0 1 1 1]\n",
            " [1 1 1 1 0 0 1 1 0 1]\n",
            " [0 1 0 1 1 1 1 1 1 1]\n",
            " [1 1 1 0 1 1 1 0 1 0]\n",
            " [0 0 0 0 1 1 0 1 1 0]\n",
            " [0 1 1 1 0 1 1 1 1 1]\n",
            " [0 1 1 1 0 0 1 1 0 1]\n",
            " [0 0 1 1 1 1 1 0 0 1]\n",
            " [0 1 1 0 1 0 0 1 1 1]\n",
            " [1 0 0 0 1 0 0 1 0 0]\n",
            " [1 0 1 1 1 0 0 1 1 0]\n",
            " [0 1 0 1 0 1 0 0 1 0]\n",
            " [1 0 1 1 0 1 0 1 1 0]\n",
            " [0 1 0 1 1 0 0 1 0 0]\n",
            " [0 0 0 1 1 1 1 0 1 1]\n",
            " [0 1 0 0 1 1 0 1 1 0]\n",
            " [0 1 1 0 1 1 1 0 1 1]\n",
            " [0 1 0 0 1 0 0 0 1 1]\n",
            " [0 1 0 1 0 0 0 0 0 0]\n",
            " [1 0 1 1 1 0 0 0 1 0]\n",
            " [0 0 1 0 1 0 0 0 0 1]\n",
            " [0 1 0 0 1 0 1 0 0 0]\n",
            " [1 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 1 0]]\n",
            "[7 5 6 5 4 5 4 7 8 7 4 8 6 6 6 3 6 4 6 4 6 5 7 4 2 5 3 3 2 2]\n",
            "(30,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T9skNDSyrAv",
        "outputId": "5b0bb66d-3339-4a48-d55d-75ea1f317c08"
      },
      "source": [
        "#################################\r\n",
        "######################################\r\n",
        "# VARIABLE DECLARATION:\r\n",
        "###################################################\r\n",
        "#basic number \r\n",
        "#T = lenght of the sequence \r\n",
        "T = 10\r\n",
        "#\r\n",
        "#I = data size \r\n",
        "I = 30\r\n",
        "#\r\n",
        "#J = output dimension = 1 not needed\r\n",
        "J = 1\r\n",
        "#\r\n",
        "#K = number of hidden neuron  not needed\r\n",
        "###################################################\r\n",
        "# data: imput X = X_train and output y =  y_train:\r\n",
        "# dimension : X  : I * T\r\n",
        "\r\n",
        "X = X_train\r\n",
        "\r\n",
        "# dimension : y : I * J = 30*1\r\n",
        "\r\n",
        "y = y_train\r\n",
        "\r\n",
        "###################################################\r\n",
        "# activation function F = identity \r\n",
        "\r\n",
        "# dimension : same as X i think \r\n",
        "# we will initialize it's value to one to start \r\n",
        "F = np.zeros((I,T))\r\n",
        "#our estimation G is the last F is the sequence this time since there is no activation function\r\n",
        "#we will initialize it to zero.\r\n",
        "\r\n",
        "###################\r\n",
        "# G for backprop\r\n",
        "G = np.zeros((I))\r\n",
        "\r\n",
        "###################################################\r\n",
        "#Weight:\r\n",
        "# Vf: weight of F \r\n",
        "#dimension : scalar\r\n",
        "# Vx:  weight of X \r\n",
        "#dimension : scalar\r\n",
        "\r\n",
        "# weight for the backprop\r\n",
        "Vxbp = np.random.uniform(0, 1)*0.001\r\n",
        "Vfbp = np.random.uniform(0, 1)*0.001\r\n",
        "\r\n",
        "# weight for the resillientprop\r\n",
        "Vx = np.random.uniform(0, 1)\r\n",
        "Vf = np.random.uniform(0, 1)\r\n",
        "\r\n",
        "##################################################\r\n",
        "#learning rate aplha\r\n",
        "#learning rate for the backprop\r\n",
        "alpha = 0.1\r\n",
        "alphabp = alpha * 0.001\r\n",
        "\r\n",
        "\r\n",
        "##################################################\r\n",
        "# resillient propagation variable:\r\n",
        "nN = 0.5\r\n",
        "nP = 1.2\r\n",
        "Dxinit = 0.001\r\n",
        "Dfinit = 0.001\r\n",
        "\r\n",
        "previousSign = 1\r\n",
        "currentSign = 1\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "print(F.shape)\r\n",
        "print(G.shape)\r\n",
        "print(y.shape)\r\n",
        "print(Vx)\r\n",
        "print(Vf)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30, 10)\n",
            "(30,)\n",
            "(30,)\n",
            "0.7654597593969069\n",
            "0.2823958439671127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evrG23c6EFib"
      },
      "source": [
        "#################################################\r\n",
        "# useful function\r\n",
        "def SSE(A,B):\r\n",
        " # E = 0.5*(((A-B)).sum())**\r\n",
        "  E = 1 / 2 * np.sum((A - B)**2)\r\n",
        "  return E\r\n",
        "\r\n",
        "def SIGN(A):\r\n",
        "  x = 0\r\n",
        "  if A >=0:\r\n",
        "    x = 1\r\n",
        "  else:\r\n",
        "    x = -1\r\n",
        "  return x "
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e25VXHTtFVmt"
      },
      "source": [
        "###################################################\r\n",
        "#Forward propagation \r\n",
        "def forwardprop(X,y,Vx,Vf,T,I,F,G):\r\n",
        "  for t in range(T-1):\r\n",
        "    for i in range(I):\r\n",
        "        F[i][t+1] = F[i][t] * Vf + X[i][t] * Vx\r\n",
        "        G[i] = F[i][T-1] * Vf + X[i][T-1] * Vx   \r\n",
        "\r\n",
        "            \r\n",
        "  \r\n",
        "  return(G)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS2kJdXbola8"
      },
      "source": [
        "###################################################\r\n",
        "# backpropagation\r\n",
        "# we have to be mindful of the difference between T \r\n",
        "# and in range(T) because in the second case it is starting from 0 to T-1\r\n",
        "# so we have to adapt our notation \r\n",
        "def backprop(G,y,Vx,Vf,T,I,alpha):\r\n",
        "  for t in range(T):\r\n",
        "    for i in range(I):\r\n",
        "      Vx = Vx - alpha * (G[i] - y[i]) * X[i][t] * (Vf**(T-(t+1)))\r\n",
        "\r\n",
        "\r\n",
        "  for t in range(T-1):    \r\n",
        "    for i in range(I):\r\n",
        "      Vf = Vf - alpha *  (G[i] - y[i]) * F[i][t] * (Vf**(T-(t+1)))\r\n",
        "\r\n",
        "\r\n",
        "      \r\n",
        "  return(Vx,Vf)\r\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLaq6d3QQ4hr"
      },
      "source": [
        "#################################################\r\n",
        "# Resillient propagation\r\n",
        "def resillientprop(G,y,Vx,Vf,T,I,nN,nP,Dxinit,Dfinit):\r\n",
        "\r\n",
        "#################################################\r\n",
        "# Vx :\r\n",
        "  Dx = Dxinit\r\n",
        "  gradX = 0\r\n",
        "  currentSIGNX = 1\r\n",
        "  previousSIGNX = 1\r\n",
        "  for t in range(T):\r\n",
        "    for i in range(I):\r\n",
        "        gradX = gradX +  (G[i] - y[i]) * X[i][t] * (Vf**(T-(t+1)))\r\n",
        "\r\n",
        "  currentSIGNX = SIGN(gradX) \r\n",
        "\r\n",
        "  if (currentSIGNX == previousSIGNX):\r\n",
        "    Dx = Dx * nP\r\n",
        "  else:\r\n",
        "    Dx = Dx * nN\r\n",
        "# update Vx\r\n",
        "\r\n",
        "  Vx = Vx - Dx * currentSIGNX\r\n",
        "  previousSIGNX = currentSIGNX \r\n",
        "\r\n",
        "#################################################\r\n",
        "# Vf :\r\n",
        "  Df = Dfinit\r\n",
        "  gradF = 0\r\n",
        "  currentSIGNF = 1\r\n",
        "  previousSIGNF = 1\r\n",
        "\r\n",
        "  for t in range(T-1):    \r\n",
        "    for i in range(I):\r\n",
        "      gradF = gradF + (G[i] - y[i]) * F[i][t] * (Vf**(T-(t+1)))\r\n",
        "  \r\n",
        "  currentSIGNF = SIGN(gradF) \r\n",
        "\r\n",
        "  if (currentSIGNF == previousSIGNF):\r\n",
        "    Df = Df * nP\r\n",
        "  else:\r\n",
        "    Df = Df * nN\r\n",
        "\r\n",
        "# update Vf\r\n",
        "  Vf = Vf - Df * currentSIGNF\r\n",
        "  previousSIGNF = currentSIGNF \r\n",
        "\r\n",
        "  return(Vx,Vf)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFuo7rhgitJY"
      },
      "source": [
        "#################################################\r\n",
        "# gradient clipping\r\n",
        "\r\n",
        "def gradientclip(G,y,Vx,Vf,T,I,N,alpha):\r\n",
        "  gradX = 0\r\n",
        "  for t in range(T):\r\n",
        "    for i in range(I):\r\n",
        "        gradX = gradX +  (G[i] - y[i]) * X[i][t] * (Vf**(T-(t+1)))\r\n",
        "\r\n",
        "  if abs(gradX) > N:\r\n",
        "    gradX = N\r\n",
        "  \r\n",
        "  Vx = Vx - (alpha * gradX)\r\n",
        "\r\n",
        "\r\n",
        "  gradF = 0\r\n",
        "  for t in range(T):\r\n",
        "    for i in range(I):\r\n",
        "        gradF = gradF +  (G[i] - y[i]) * F[i][t] * (Vf**(T-(t+1)))\r\n",
        "\r\n",
        "\r\n",
        "  if abs(gradF) > N:\r\n",
        "    gradF = N\r\n",
        "  \r\n",
        "  Vf = Vf - (alpha * gradF)\r\n",
        "\r\n",
        "  return(Vx,Vf)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NA-H4H15sG5l",
        "outputId": "3b0b388c-06a8-4bd7-f0c3-d54ed52fef3a"
      },
      "source": [
        "############################################\r\n",
        "#RNN with backprop\r\n",
        "\r\n",
        "# we re-initialize G and F so you can launch the other propagation \r\n",
        "F = np.zeros((I,T))\r\n",
        "G = np.zeros((I))\r\n",
        "\r\n",
        "# weight for the backprop\r\n",
        "Vxbp = np.random.uniform(0, 1)*0.001\r\n",
        "Vfbp = np.random.uniform(0, 1)*0.001\r\n",
        "\r\n",
        "\r\n",
        "# the backpropagation converge for extremely small value of alpha \r\n",
        "#and extremely small weight initializiation\r\n",
        "print(\"Error before using our RNN with backpropragation\", SSE(y,G))\r\n",
        "for i in range(2000):\r\n",
        "  G = forwardprop(X,y,Vxbp,Vfbp,T,I,F,G)        \r\n",
        "  (Vxbp,Vfbp) = backprop(G,y,Vxbp,Vfbp,T,I,alphabp)\r\n",
        "print(\"Error after using our RNN with backpropragation\",SSE(y,G))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error before using our RNN with backpropragation 418.0\n",
            "Error after using our RNN with backpropragation 0.49317710278688837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGKTXCUPxGhz",
        "outputId": "25243635-aee2-41b3-bc9d-f616225553ed"
      },
      "source": [
        "############################################\r\n",
        "# RNN with resillient prop\r\n",
        "F = np.zeros((I,T))\r\n",
        "G = np.zeros((I))\r\n",
        "\r\n",
        "\r\n",
        "# weight for the resillientprop\r\n",
        "Vx = np.random.uniform(0, 1)\r\n",
        "Vf = np.random.uniform(0, 1)\r\n",
        "\r\n",
        "print(\"Error before using our RNN with resillient propragation\", SSE(y,G))\r\n",
        "for i in range(2000):\r\n",
        "  G = forwardprop(X,y,Vx,Vf,T,I,F,G)        \r\n",
        "  (Vx,Vf) = resillientprop(G,y,Vx,Vf,T,I,nN,nP,Dxinit,Dfinit)\r\n",
        "print(\"Error after using our RNN with resillient backpropragation\",SSE(y,G))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error before using our RNN with resillient propragation 418.0\n",
            "Error after using our RNN with resillient backpropragation 0.017584838879619178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSilmybLq3uW",
        "outputId": "9cc7d4fd-86c8-4e92-a9e5-19c15142a125"
      },
      "source": [
        "############################################\r\n",
        "#RNN with gradient clipping\r\n",
        "\r\n",
        "# we re-initialize G and F so you can launch the other propagation \r\n",
        "F = np.zeros((I,T))\r\n",
        "G = np.zeros((I))\r\n",
        "\r\n",
        "# learning rate alpha\r\n",
        "alpha = 0.1\r\n",
        "\r\n",
        "# value for clipping N:\r\n",
        "N = 0.3\r\n",
        "\r\n",
        "# weight for the backprop\r\n",
        "Vxclip = np.random.uniform(0, 1)\r\n",
        "Vfclip = np.random.uniform(0, 1)\r\n",
        "\r\n",
        "\r\n",
        "# the backpropagation converge for extremely small value of alpha \r\n",
        "#and extremely small weight initializiation\r\n",
        "print(\"Error before using our RNN with backpropragation\", SSE(y,G))\r\n",
        "for i in range(2000):\r\n",
        "  G = forwardprop(X,y,Vxbp,Vfbp,T,I,F,G)        \r\n",
        "  (Vxclip,Vfclip) = gradientclip(G,y,Vxbp,Vfbp,T,I,N,alpha)\r\n",
        "print(\"Error after using our RNN with backpropragation\",SSE(y,G))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error before using our RNN with backpropragation 418.0\n",
            "Error after using our RNN with backpropragation 0.4907845975450945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3ypQjwVm-u2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}