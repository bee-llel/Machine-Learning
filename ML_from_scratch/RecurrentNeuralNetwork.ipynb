{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RecurrentNeuralNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwtVM/gJ/7N4rVJIkJr2Cj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bee-llel/Machine-Learning/blob/master/RecurrentNeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0mapvIp7xp9"
      },
      "source": [
        "#librairy:\r\n",
        "import pandas as pd # used to manipulate the data \r\n",
        "import numpy as np\r\n",
        "import scipy as sp \r\n",
        "from numpy import *\r\n"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPAN-qGVBHYo"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F7R-kTtxyYH",
        "outputId": "635d8295-ce36-4f7f-a6f2-a5c2e7b9229a"
      },
      "source": [
        "import random as rd \r\n",
        "from random import *\r\n",
        "#allows us to always have the same result when calling a random function\r\n",
        "np.random.seed(23)\r\n",
        "# this small part was made with another group in class ( it's the lab) so it's the same as a few classmate.\r\n",
        "\r\n",
        "#######################################\r\n",
        "# DATA : random binary number \r\n",
        "\r\n",
        "X_train = []\r\n",
        "X_test = [] \r\n",
        "y_train = []\r\n",
        "y_test = [] \r\n",
        "\r\n",
        "for i in range(30) :\r\n",
        "  X = [randint(0, 1) for i in range(10) ]\r\n",
        "  X_train.append(X)\r\n",
        "  y_train.append(sum(X))\r\n",
        "\r\n",
        "for i in range(10):\r\n",
        "  X = [randint(0, 1) for i in range(10) ]\r\n",
        "  X_test.append(X)\r\n",
        "  y_test.append(sum(X))\r\n",
        "\r\n",
        "X_train = np.asarray(X_train)\r\n",
        "X_test = np.asarray(X_test)\r\n",
        "y_train = np.asarray(y_train)\r\n",
        "y_test = np.asarray(y_test)\r\n",
        "\r\n",
        "y_train.reshape((30,1))\r\n",
        "print(X_train)\r\n",
        "print(y_train)\r\n",
        "print(y_train.shape)"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 0 1 0 0 1 0 0 1]\n",
            " [0 0 1 1 1 0 0 1 0 0]\n",
            " [0 1 1 0 0 1 1 1 1 0]\n",
            " [0 1 1 1 0 0 0 0 0 1]\n",
            " [0 1 0 0 1 0 0 1 0 0]\n",
            " [1 0 0 0 1 1 0 0 1 0]\n",
            " [0 0 0 1 1 0 0 0 0 1]\n",
            " [1 1 0 1 0 1 0 1 0 1]\n",
            " [1 0 1 0 0 0 1 1 0 0]\n",
            " [1 1 1 1 1 1 0 0 0 1]\n",
            " [0 0 0 0 0 1 0 0 0 1]\n",
            " [1 1 1 1 0 0 1 1 1 0]\n",
            " [1 0 0 1 0 0 1 1 0 0]\n",
            " [1 0 0 1 0 1 1 0 1 0]\n",
            " [1 0 1 1 1 0 0 1 1 0]\n",
            " [1 1 0 0 1 1 1 1 1 1]\n",
            " [0 1 1 1 0 1 0 1 1 1]\n",
            " [0 1 1 0 1 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 1 1 0 0 1 1]\n",
            " [0 0 1 1 1 0 0 1 1 1]\n",
            " [1 1 1 1 0 0 0 0 0 1]\n",
            " [1 0 1 1 0 1 0 0 1 0]\n",
            " [0 0 0 1 0 0 1 0 0 0]\n",
            " [1 1 1 0 0 0 1 0 1 0]\n",
            " [1 0 0 0 1 0 1 1 1 0]\n",
            " [1 1 0 0 1 0 0 1 0 1]\n",
            " [1 1 1 1 1 0 0 0 1 1]\n",
            " [1 1 0 0 0 0 0 0 1 0]\n",
            " [0 0 1 1 1 0 0 1 1 1]]\n",
            "[5 4 6 4 3 4 3 6 4 7 2 7 4 5 6 8 7 3 2 4 6 5 5 2 5 5 5 7 3 6]\n",
            "(30,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T9skNDSyrAv",
        "outputId": "55c513a9-e4cf-4c96-a53f-975ffe355a6b"
      },
      "source": [
        "#################################\r\n",
        "######################################\r\n",
        "# VARIABLE DECLARATION:\r\n",
        "###################################################\r\n",
        "#basic number \r\n",
        "#T = lenght of the sequence \r\n",
        "T = 10\r\n",
        "#\r\n",
        "#I = data size \r\n",
        "I = 30\r\n",
        "#\r\n",
        "#J = output dimension = 1 not needed\r\n",
        "J = 1\r\n",
        "#\r\n",
        "#K = number of hidden neuron  not needed\r\n",
        "###################################################\r\n",
        "# data: imput X = X_train and output y =  y_train:\r\n",
        "# dimension : X  : I * T\r\n",
        "\r\n",
        "X = X_train\r\n",
        "\r\n",
        "# dimension : y : I * J = 30*1\r\n",
        "\r\n",
        "y = y_train\r\n",
        "\r\n",
        "###################################################\r\n",
        "# activation function F = identity \r\n",
        "\r\n",
        "# dimension : same as X i think \r\n",
        "# we will initialize it's value to one to start \r\n",
        "F = np.zeros((I,T))\r\n",
        "#our estimation G is the last F is the sequence this time since there is no activation function\r\n",
        "#we will initialize it to zero.\r\n",
        "\r\n",
        "###################\r\n",
        "# G for backprop\r\n",
        "G = np.zeros((I))\r\n",
        "\r\n",
        "###################################################\r\n",
        "#Weight:\r\n",
        "# Vf: weight of F \r\n",
        "#dimension : scalar\r\n",
        "# Vx:  weight of X \r\n",
        "#dimension : scalar\r\n",
        "\r\n",
        "# weight for the backprop\r\n",
        "Vxbp = np.random.uniform(0, 1)*0.001\r\n",
        "Vfbp = np.random.uniform(0, 1)*0.001\r\n",
        "\r\n",
        "# weight for the resillientprop\r\n",
        "Vx = np.random.uniform(0, 1)\r\n",
        "Vf = np.random.uniform(0, 1)\r\n",
        "\r\n",
        "##################################################\r\n",
        "#learning rate aplha\r\n",
        "#learning rate for the backprop\r\n",
        "alpha = 0.001\r\n",
        "alphabp = alpha \r\n",
        "\r\n",
        "\r\n",
        "##################################################\r\n",
        "# resillient propagation variable:\r\n",
        "nN = 0.5\r\n",
        "nP = 1.2\r\n",
        "Dxinit = 0.001\r\n",
        "Dfinit = 0.001\r\n",
        "\r\n",
        "previousSign = 1\r\n",
        "currentSign = 1\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "print(F.shape)\r\n",
        "print(G.shape)\r\n",
        "print(y.shape)\r\n",
        "print(Vx)\r\n",
        "print(Vf)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30, 10)\n",
            "(30,)\n",
            "(30,)\n",
            "0.7654597593969069\n",
            "0.2823958439671127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evrG23c6EFib"
      },
      "source": [
        "#################################################\r\n",
        "# useful function\r\n",
        "def SSE(A,B):\r\n",
        " # E = 0.5*(((A-B)).sum())**\r\n",
        "  E = 1 / 2 * np.sum((A - B)**2)\r\n",
        "  return E\r\n",
        "\r\n",
        "def SIGN(A):\r\n",
        "  x = 0\r\n",
        "  if A >=0:\r\n",
        "    x = 1\r\n",
        "  else:\r\n",
        "    x = -1\r\n",
        "  return x "
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sKCg52HrAkr"
      },
      "source": [
        "Forward propagation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e25VXHTtFVmt"
      },
      "source": [
        "###################################################\r\n",
        "#Forward propagation \r\n",
        "def forwardprop(X,y,Vx,Vf,T,I,F,G):\r\n",
        "  for t in range(T-1):\r\n",
        "    for i in range(I):\r\n",
        "        F[i][t+1] = F[i][t] * Vf + X[i][t] * Vx\r\n",
        "        G[i] = F[i][T-1] * Vf + X[i][T-1] * Vx   \r\n",
        "\r\n",
        "            \r\n",
        "  \r\n",
        "  return(G)"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyIBmm9NrGvP"
      },
      "source": [
        "Propagation function and gradient clipping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS2kJdXbola8"
      },
      "source": [
        "###################################################\r\n",
        "# backpropagation\r\n",
        "# we have to be mindful of the difference between T \r\n",
        "# and in range(T) because in the second case it is starting from 0 to T-1\r\n",
        "# so we have to adapt our notation \r\n",
        "def backprop(G,y,Vx,Vf,T,I,alpha):\r\n",
        "  for t in range(T):\r\n",
        "    for i in range(I):\r\n",
        "      Vx = Vx - alpha * (G[i] - y[i]) * X[i][t] * (Vf**(T-(t+1)))\r\n",
        "\r\n",
        "\r\n",
        "  for t in range(T-1):    \r\n",
        "    for i in range(I):\r\n",
        "      Vf = Vf - alpha *  (G[i] - y[i]) * F[i][t] * (Vf**(T-(t+1)))\r\n",
        "\r\n",
        "\r\n",
        "      \r\n",
        "  return(Vx,Vf)\r\n"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLaq6d3QQ4hr"
      },
      "source": [
        "#################################################\r\n",
        "# Resillient propagation\r\n",
        "def resillientprop(G,y,Vx,Vf,T,I,nN,nP,Dxinit,Dfinit):\r\n",
        "\r\n",
        "#################################################\r\n",
        "# Vx :\r\n",
        "  Dx = Dxinit\r\n",
        "  gradX = 0\r\n",
        "  currentSIGNX = 1\r\n",
        "  previousSIGNX = 1\r\n",
        "  for t in range(T):\r\n",
        "    for i in range(I):\r\n",
        "        gradX = gradX +  (G[i] - y[i]) * X[i][t] * (Vf**(T-(t+1)))\r\n",
        "\r\n",
        "  currentSIGNX = SIGN(gradX) \r\n",
        "\r\n",
        "  if (currentSIGNX == previousSIGNX):\r\n",
        "    Dx = Dx * nP\r\n",
        "  else:\r\n",
        "    Dx = Dx * nN\r\n",
        "# update Vx\r\n",
        "\r\n",
        "  Vx = Vx - Dx * currentSIGNX\r\n",
        "  previousSIGNX = currentSIGNX \r\n",
        "\r\n",
        "#################################################\r\n",
        "# Vf :\r\n",
        "  Df = Dfinit\r\n",
        "  gradF = 0\r\n",
        "  currentSIGNF = 1\r\n",
        "  previousSIGNF = 1\r\n",
        "\r\n",
        "  for t in range(T-1):    \r\n",
        "    for i in range(I):\r\n",
        "      gradF = gradF + (G[i] - y[i]) * F[i][t] * (Vf**(T-(t+1)))\r\n",
        "  \r\n",
        "  currentSIGNF = SIGN(gradF) \r\n",
        "\r\n",
        "  if (currentSIGNF == previousSIGNF):\r\n",
        "    Df = Df * nP\r\n",
        "  else:\r\n",
        "    Df = Df * nN\r\n",
        "\r\n",
        "# update Vf\r\n",
        "  Vf = Vf - Df * currentSIGNF\r\n",
        "  previousSIGNF = currentSIGNF \r\n",
        "\r\n",
        "  return(Vx,Vf)"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFuo7rhgitJY"
      },
      "source": [
        "#################################################\r\n",
        "# gradient clipping\r\n",
        "\r\n",
        "def gradientclip(G,y,Vx,Vf,T,I,N,alpha):\r\n",
        "  gradX = 0\r\n",
        "  for t in range(T):\r\n",
        "    for i in range(I):\r\n",
        "        gradX = gradX +  (G[i] - y[i]) * X[i][t] * (Vf**(T-(t+1)))\r\n",
        "\r\n",
        "  if abs(gradX) > N:\r\n",
        "    gradX = N\r\n",
        "  \r\n",
        "  Vx = Vx - (alpha * gradX)\r\n",
        "\r\n",
        "\r\n",
        "  gradF = 0\r\n",
        "  for t in range(T-1):\r\n",
        "    for i in range(I):\r\n",
        "        gradF = gradF +  (G[i] - y[i]) * F[i][t] * (Vf**(T-(t+1)))\r\n",
        "\r\n",
        "\r\n",
        "  if abs(gradF) > N:\r\n",
        "    gradF = N\r\n",
        "  \r\n",
        "  Vf = Vf - (alpha * gradF)\r\n",
        "\r\n",
        "  return(Vx,Vf)"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DHRjGMyrOmB"
      },
      "source": [
        "Convergence demonstration for each method using different value :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NA-H4H15sG5l",
        "outputId": "b4344ee3-db1c-4529-ceda-81b96f1559b0"
      },
      "source": [
        "############################################\r\n",
        "#RNN with backprop\r\n",
        "\r\n",
        "# we re-initialize G and F so you can launch the other propagation \r\n",
        "F = np.zeros((I,T))\r\n",
        "G = np.zeros((I))\r\n",
        "\r\n",
        "# weight for the backprop\r\n",
        "Vxbp = np.random.uniform(0, 1)*0.001\r\n",
        "Vfbp = np.random.uniform(0, 1)*0.001\r\n",
        "\r\n",
        "\r\n",
        "# the backpropagation converge for extremely small value of alpha \r\n",
        "#and extremely small weight initializiation\r\n",
        "print(\"Error before using our RNN with backpropragation\", SSE(y,G))\r\n",
        "for i in range(2000):\r\n",
        "  G = forwardprop(X,y,Vxbp,Vfbp,T,I,F,G)        \r\n",
        "  (Vxbp,Vfbp) = backprop(G,y,Vxbp,Vfbp,T,I,alphabp)\r\n",
        "print(\"Error after using our RNN with backpropragation\",SSE(y,G))"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error before using our RNN with backpropragation 379.5\n",
            "Error after using our RNN with backpropragation 97.19454155915344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGKTXCUPxGhz",
        "outputId": "7461b47f-e332-4a22-cbc9-92e87b853fd9"
      },
      "source": [
        "############################################\r\n",
        "# RNN with resillient prop\r\n",
        "F = np.zeros((I,T))\r\n",
        "G = np.zeros((I))\r\n",
        "\r\n",
        "\r\n",
        "# weight for the resillientprop\r\n",
        "Vx = np.random.uniform(0, 1)\r\n",
        "Vf = np.random.uniform(0, 1)\r\n",
        "\r\n",
        "print(\"Error before using our RNN with resillient propragation\", SSE(y,G))\r\n",
        "for i in range(2000):\r\n",
        "  G = forwardprop(X,y,Vx,Vf,T,I,F,G)        \r\n",
        "  (Vx,Vf) = resillientprop(G,y,Vx,Vf,T,I,nN,nP,Dxinit,Dfinit)\r\n",
        "print(\"Error after using our RNN with resillient backpropragation\",SSE(y,G))"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error before using our RNN with resillient propragation 379.5\n",
            "Error after using our RNN with resillient backpropragation 0.0031161020805432985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSilmybLq3uW",
        "outputId": "5d5de528-887b-4343-cc8c-965f1ea48346"
      },
      "source": [
        "############################################\r\n",
        "#RNN with gradient clipping\r\n",
        "\r\n",
        "# we re-initialize G and F so you can launch the other propagation \r\n",
        "F = np.zeros((I,T))\r\n",
        "G = np.zeros((I))\r\n",
        "\r\n",
        "# learning rate alpha\r\n",
        "alpha = 0.01\r\n",
        "\r\n",
        "# value for clipping N:\r\n",
        "N = 0.02\r\n",
        "\r\n",
        "# weight for the backprop\r\n",
        "Vxclip = np.random.uniform(0, 1)\r\n",
        "Vfclip = np.random.uniform(0, 1)\r\n",
        "\r\n",
        "# the gradient clipping converge for greater value compared to backpropagation\r\n",
        "print(\"Error before using our RNN with gradient clipping\", SSE(y,G))\r\n",
        "for i in range(2000):\r\n",
        "  G = forwardprop(X,y,Vxclip,Vfclip,T,I,F,G)        \r\n",
        "  (Vxclip,Vfclip) = gradientclip(G,y,Vxclip,Vfclip,T,I,N,alpha)\r\n",
        "print(\"Error after using our RNN with gradient clipping\",SSE(y,G))"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error before using our RNN with gradient clipping 379.5\n",
            "Error after using our RNN with gradient clipping 339.39998267541824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keK14rOfsEpj"
      },
      "source": [
        "Comparaison using same initial value for the three method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3ypQjwVm-u2",
        "outputId": "f42ac932-fbca-4045-d635-7982c3d91b40"
      },
      "source": [
        "#############################\r\n",
        "F = np.zeros((I,T))\r\n",
        "G = np.zeros((I))\r\n",
        "nN = 0.5\r\n",
        "nP = 1.2\r\n",
        "Dxinit = 0.001\r\n",
        "Dfinit = 0.001\r\n",
        "N = 0.02\r\n",
        "alpha = 0.001 \r\n",
        "U = np.random.uniform(0, 1)\r\n",
        "V = np.random.uniform(0, 1)\r\n",
        "\r\n",
        "Vxbp = U\r\n",
        "Vfbp = V\r\n",
        "\r\n",
        "Vx = U\r\n",
        "Vf = V\r\n",
        "\r\n",
        "Vxclip = np.random.uniform(0, 1)\r\n",
        "Vfclip = np.random.uniform(0, 1)\r\n",
        "\r\n",
        "G1 = np.zeros((I))\r\n",
        "G2 = np.zeros((I))\r\n",
        "G3 = np.zeros((I))\r\n",
        "\r\n",
        "F1 = np.zeros((I,T))\r\n",
        "F2 = np.zeros((I,T))\r\n",
        "F3 = np.zeros((I,T))\r\n",
        "print(\"Error before using our RNN with backpropragation\", SSE(y,G1))\r\n",
        "for i in range(10):\r\n",
        "  G1 = forwardprop(X,y,Vxbp,Vfbp,T,I,F,G1)        \r\n",
        "  (Vxbp,Vfbp) = backprop(G1,y,Vxbp,Vfbp,T,I,alpha)\r\n",
        "print(\"Error after using our RNN with backpropragation\",SSE(y,G1))\r\n",
        "\r\n",
        "\r\n",
        "print(\"Error before using our RNN with resillient propragation\", SSE(y,G2))\r\n",
        "for i in range(2000):\r\n",
        "  G2 = forwardprop(X,y,Vx,Vf,T,I,F,G2)        \r\n",
        "  (Vx,Vf) = resillientprop(G2,y,Vx,Vf,T,I,nN,nP,Dxinit,Dfinit)\r\n",
        "print(\"Error after using our RNN with resillient propragation\",SSE(y,G2))\r\n",
        "\r\n",
        "\r\n",
        "print(\"Error before using our RNN with gradient clipping\", SSE(y,G3))\r\n",
        "for i in range(2000):\r\n",
        "  G3 = forwardprop(X,y,Vxclip,Vfclip,T,I,F,G3)        \r\n",
        "  (Vxclip,Vfclip) = gradientclip(G3,y,Vxclip,Vfclip,T,I,N,alpha)\r\n",
        "                    \r\n",
        "print(\"Error after using our RNN with gradient clipping\",SSE(y,G3))\r\n"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error before using our RNN with backpropragation 379.5\n",
            "Error after using our RNN with backpropragation nan\n",
            "Error before using our RNN with resillient propragation 379.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: overflow encountered in double_scalars\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Error after using our RNN with resillient propragation 0.010443119066112803\n",
            "Error before using our RNN with gradient clipping 379.5\n",
            "Error after using our RNN with gradient clipping 299.1081091588626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40pBqTDdt4AK",
        "outputId": "4e018c72-b183-47d0-944d-a81dda9d6d5b"
      },
      "source": [
        "print(\"We can see that backpropagation is extremely unstable and will easily diverge\")\r\n",
        "print(\"We can see that resillient propagation both converge with far greater value than for backpropagation\")\r\n",
        "print(\"But we can see that resillient progation converge way faster than gradient clipping in my example\")\r\n",
        "print(\"Gradient clipping seem to really depend on the initial value like the backpropagation although it is more robust\")"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We can see that backpropagation is extremely unstable and will easily diverge\n",
            "We can see that resillient propagation both converge with far greater value than for backpropagation\n",
            "But we can see that resillient progation converge way faster than gradient clipping in my example\n",
            "Gradient clipping seem to really depend on the initial value like the backpropagation although it is more robust\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nREOGv5Z2dgH"
      },
      "source": [
        "using test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz3laLkBwoVc",
        "outputId": "567067fa-7664-4c84-e3f5-e0ec3f1b39f7"
      },
      "source": [
        "G1 = np.zeros((10))\r\n",
        "G2 = np.zeros((10))\r\n",
        "G3 = np.zeros((10))\r\n",
        "X = X_test\r\n",
        "y = y_test\r\n",
        "F = np.zeros((10,T))\r\n",
        "alpha = 0.001\r\n",
        "\r\n",
        "Vxbp = np.random.uniform(0, 1)\r\n",
        "Vfbp = np.random.uniform(0, 1)\r\n",
        "\r\n",
        "Vx = np.random.uniform(0, 1)\r\n",
        "Vf = np.random.uniform(0, 1)\r\n",
        "\r\n",
        "\r\n",
        "Vxclip = np.random.uniform(0, 1)\r\n",
        "Vfclip = np.random.uniform(0, 1)\r\n",
        "\r\n",
        "print(\"Error before using our RNN with backpropragation\", SSE(y,G1))\r\n",
        "for i in range(10):\r\n",
        "  G1 = forwardprop(X,y,Vxbp,Vfbp,T,10,F,G1)        \r\n",
        "  (Vxbp,Vfbp) = backprop(G1,y,Vxbp,Vfbp,T,10,alpha)\r\n",
        "print(\"Error after using our RNN with backpropragation\",SSE(y,G1))\r\n",
        "\r\n",
        "print(\"Error before using our RNN with resillient propragation\", SSE(y,G2))\r\n",
        "for i in range(2000):\r\n",
        "  G2 = forwardprop(X,y,Vx,Vf,T,10,F,G2)        \r\n",
        "  (Vx,Vf) = resillientprop(G2,y,Vx,Vf,T,10,nN,nP,Dxinit,Dfinit)\r\n",
        "print(\"Error after using our RNN with resillient propragation\",SSE(y,G2))\r\n",
        "\r\n",
        "print(\"Error before using our RNN with gradient clipping\", SSE(y,G3))\r\n",
        "for i in range(2000):\r\n",
        "  G3 = forwardprop(X,y,Vxclip,Vfclip,T,10,F,G3)        \r\n",
        "  (Vxclip,Vfclip) = gradientclip(G3,y,Vxclip,Vfclip,T,10,N,alpha)\r\n",
        "                    \r\n",
        "print(\"Error after using our RNN with gradient clipping\",SSE(y,G3))"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error before using our RNN with backpropragation 130.5\n",
            "Error after using our RNN with backpropragation 84.44263898985066\n",
            "Error before using our RNN with resillient propragation 130.5\n",
            "Error after using our RNN with resillient propragation 0.0014434388421663857\n",
            "Error before using our RNN with gradient clipping 130.5\n",
            "Error after using our RNN with gradient clipping 92.42460220205942\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzdNRCu3xH8C",
        "outputId": "ab1dcab4-a2d2-45ed-ec82-bcbb21f233ce"
      },
      "source": [
        "print(\" we can see that with test value resillient gradient stay the best method we have\")"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " we can see that with test value resillient gradient stay the best method we have\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScYaessg0xOk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}